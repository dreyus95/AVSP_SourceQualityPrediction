{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0_14_16505_55.cpp,1,49,13,47,0,2,23.50,0.00,23.50,0.96,47,0,2\r\n",
      "\n",
      "0.0_14_16505_55.csv;1;43;43.000;12;12.000;0;0.000;******;******;0;0.000;0;0.000;0;0.000;3\n",
      "\n",
      "#file_name;cnt_classes;max_member_funs;max_nested_loops;max_nesting_depth;max_params_in_decl;member_funs;member_vars;min_member_funs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "linesCodeAnalyzer = None\n",
    "with open('CodeJamCrawler/dataset_csvs/code_analyzer_out_sorted.csv', 'r') as f:\n",
    "    linesCodeAnalyzer = f.readlines()\n",
    "    \n",
    "codeAnalyzerHeader = linesCodeAnalyzer[0]\n",
    "linesCodeAnalyzer = linesCodeAnalyzer[1:]\n",
    "\n",
    "linesCccc = None\n",
    "with open('CodeJamCrawler/dataset_csvs/cccc_cleared.csv', 'r') as f:\n",
    "    linesCccc = f.readlines()\n",
    "    \n",
    "linesASTExtractor = None\n",
    "with open('ASTExtractor/ast_features.csv', 'r') as f:\n",
    "    linesASTExtractor = f.readlines()\n",
    "\n",
    "ASTExtractorHeader = linesASTExtractor[0]\n",
    "    \n",
    "ccccHeader = linesCccc[0]\n",
    "linesCccc = linesCccc[1:]\n",
    "linesASTExtractor = linesASTExtractor[1:]\n",
    "\n",
    "print (linesCodeAnalyzer[0])\n",
    "print (linesCccc[0])\n",
    "print (ASTExtractorHeader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def get_file_name(line, splitter):\n",
    "    entries = line.split(splitter)\n",
    "    file_name, file_ext = os.path.splitext(entries[0])\n",
    "    return file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0_14_16505_55\n",
      "0.0_14_16505_55\n",
      "0.0_14_16505_55\n"
     ]
    }
   ],
   "source": [
    "print (get_file_name(linesCodeAnalyzer[0], ','))\n",
    "print (get_file_name(linesCccc[0], ';'))\n",
    "print (get_file_name(linesASTExtractor[0], ';'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "('Number of cpp files:', 18667)\n"
     ]
    }
   ],
   "source": [
    "# removing all the entries that aren't existing in outputCleared\n",
    "cpp_files_only = []\n",
    "for i in range(len(linesCodeAnalyzer)):\n",
    "    if i % 1000 == 0:\n",
    "        print (i)\n",
    "        \n",
    "    other = linesCodeAnalyzer[i]\n",
    "    file_name_other = get_file_name(other, ',')\n",
    "    \n",
    "    found = False\n",
    "    for cleared in linesCccc:\n",
    "        file_name_cleared = get_file_name(cleared, ';')\n",
    "        if file_name_cleared == file_name_other:\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        continue\n",
    "    cpp_files_only.append(file_name_other)\n",
    "    \n",
    "print (\"Number of cpp files:\", len(cpp_files_only))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0_14_16505_55\n"
     ]
    }
   ],
   "source": [
    "print (cpp_files_only[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Code analyzer output:', '#File Name,Files,Lines,AVG Len,Code,Comments,White SP,Cd/Cm+WS,Cd/Cm,Cd/WS,% Code,Cd/File,Cm/File,WS/File,\\r\\n')\n",
      "('CCCC analyzer output:', '#filename;number_of_modules;lines_of_code;lines_of_code_per_module;McCabes_cyclomatic_complexity;McCabes_cyclomatic_complexity_per_module;lines_of_comment;lines_of_comment_per_module;lines_of_code_per_line_of_comment;McCabes_cyclomatic_complexity_per_line_of_comment;IF4;IF4_per_module;IF4_visible;IF4_visible_per_module;IF4_concrete;IF4_concrete;rejected_lines_of_code\\n')\n"
     ]
    }
   ],
   "source": [
    "print ('Code analyzer output:', codeAnalyzerHeader)\n",
    "print ('CCCC analyzer output:', ccccHeader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def parse_cccc(cccc_line):\n",
    "    name = get_file_name(cccc_line, ';')\n",
    "    res = name.split('_')[-1]\n",
    "    author = name\n",
    "    features = []\n",
    "    for num in cccc_line.split(';')[1:]:\n",
    "        if num == '******' or num == '------':\n",
    "            num = '0.0'\n",
    "        try:\n",
    "            features.append(float(num))\n",
    "        except ValueError:\n",
    "            print (num)\n",
    "            break\n",
    "    return author, features, res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_code_analyzer(code_anal_line):\n",
    "    name = get_file_name(code_anal_line, ',')\n",
    "    res = name.split('_')[-1]\n",
    "    author = name\n",
    "    features = []\n",
    "    for num in code_anal_line.split(',')[1:]:\n",
    "        try:\n",
    "            features.append(float(num))\n",
    "        except ValueError:\n",
    "            print (num)\n",
    "            break\n",
    "    return author, features, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_ast_extractor(code_extractor_line):\n",
    "    name = get_file_name(code_extractor_line, ';')\n",
    "    res = name.split('_')[-1]\n",
    "    author = name#.split('_')[0]\n",
    "    features = []\n",
    "    for num in code_extractor_line.split(';')[1:]:\n",
    "        try:\n",
    "            features.append(float(num))\n",
    "        except ValueError:\n",
    "            print (num)\n",
    "            break\n",
    "    return author, features, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('cccc line vec', ('0.0_14_16505_55', [1.0, 43.0, 43.0, 12.0, 12.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0], '55'))\n",
      "('code line vec', ('0.0_14_16505_55', [1.0, 49.0, 13.0, 47.0, 0.0, 2.0, 23.5, 0.0, 23.5, 0.96, 47.0, 0.0, 2.0], '55'))\n",
      "('ast line vec', ('0.0_14_16505_55', [0.0, 0.0, 3.0, 4.0, 0.0, 0.0, 0.0, 150000.0], '55'))\n"
     ]
    }
   ],
   "source": [
    "print ('cccc line vec', parse_cccc(linesCccc[0]))\n",
    "print ('code line vec', parse_code_analyzer(linesCodeAnalyzer[0]))\n",
    "print ('ast line vec', parse_ast_extractor(linesASTExtractor[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18667, 18667, 18667)\n"
     ]
    }
   ],
   "source": [
    "cpp_files_map = set()\n",
    "\n",
    "for n in cpp_files_only:\n",
    "    cpp_files_map.add(n)\n",
    "\n",
    "src_file_vec_map = {}\n",
    "\n",
    "for cccc_line in linesCccc:\n",
    "    name, features, res = parse_cccc(cccc_line)\n",
    "    if name not in src_file_vec_map and name in cpp_files_map:\n",
    "        src_file_vec_map[name] = [(features,res)]\n",
    "    elif name in cpp_files_map:\n",
    "        src_file_vec_map[name].append((features, res))\n",
    "\n",
    "for code_anal_line in linesCodeAnalyzer:\n",
    "    name, features, res = parse_code_analyzer(code_anal_line)\n",
    "    if name in cpp_files_map:\n",
    "        src_file_vec_map[name].append((features, res))\n",
    "\n",
    "i = 0\n",
    "for ast_line in linesASTExtractor:\n",
    "    name, features, res = parse_ast_extractor(ast_line)\n",
    "    if name in cpp_files_map:\n",
    "        src_file_vec_map[name].append((features, res))\n",
    "        i += 1\n",
    "        \n",
    "print (i, len(cpp_files_only), len(cpp_files_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_author_name(file_name):\n",
    "    return file_name.split('_')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src_vec_map = {}\n",
    "\n",
    "for key in src_file_vec_map.keys():\n",
    "    v1 = src_file_vec_map[key][0]\n",
    "    v2 = src_file_vec_map[key][1]\n",
    "    v3 = src_file_vec_map[key][2]\n",
    "    author = get_author_name(key)\n",
    "    #print key\n",
    "    if author not in src_vec_map.keys():\n",
    "        src_vec_map[author] = [(v1,v2,v3)]\n",
    "    else:\n",
    "        src_vec_map[author].append((v1, v2, v3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "(([1.0, 85.0, 85.0, 18.0, 18.0, 47.0, 47.0, 1.809, 0.383, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0], '55'), ([1.0, 160.0, 16.0, 110.0, 47.0, 3.0, 2.2, 2.34, 36.67, 0.69, 110.0, 47.0, 3.0], '55'), ([0.0, 0.0, 3.0, 4.0, 0.0, 0.0, 0.0, 150000.0], '55'))\n",
      "(([1.0, 53.0, 53.0, 24.0, 24.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0], '80'), ([1.0, 86.0, 19.0, 83.0, 0.0, 3.0, 27.67, 0.0, 27.67, 0.97, 83.0, 0.0, 3.0], '80'), ([0.0, 0.0, 4.0, 5.0, 0.0, 0.0, 0.0, 150000.0], '80'))\n",
      "(([1.0, 44.0, 44.0, 14.0, 14.0, 1.0, 1.0, 44.0, 14.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0], '60'), ([1.0, 78.0, 18.0, 74.0, 1.0, 3.0, 18.5, 74.0, 24.67, 0.95, 74.0, 1.0, 3.0], '60'), ([0.0, 0.0, 2.0, 3.0, 0.0, 0.0, 0.0, 150000.0], '60'))\n",
      "(([1.0, 47.0, 47.0, 10.0, 10.0, 46.0, 46.0, 1.022, 0.217, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], '55'), ([1.0, 121.0, 16.0, 72.0, 46.0, 3.0, 1.47, 1.57, 24.0, 0.6, 72.0, 46.0, 3.0], '55'), ([0.0, 0.0, 3.0, 4.0, 0.0, 0.0, 0.0, 150000.0], '55'))\n",
      "(([1.0, 28.0, 28.0, 4.0, 4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], '55'), ([1.0, 54.0, 16.0, 53.0, 0.0, 1.0, 53.0, 0.0, 53.0, 0.98, 53.0, 0.0, 1.0], '55'), ([0.0, 0.0, 2.0, 3.0, 0.0, 0.0, 0.0, 150000.0], '55'))\n",
      "(([1.0, 74.0, 74.0, 16.0, 16.0, 47.0, 47.0, 1.574, 0.34, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], '55'), ([1.0, 149.0, 15.0, 99.0, 47.0, 3.0, 1.98, 2.11, 33.0, 0.66, 99.0, 47.0, 3.0], '55'), ([0.0, 0.0, 2.0, 3.0, 0.0, 0.0, 0.0, 150000.0], '55'))\n",
      "(([1.0, 97.0, 97.0, 18.0, 18.0, 1.0, 1.0, 97.0, 18.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8.0], '20'), ([1.0, 127.0, 14.0, 123.0, 1.0, 3.0, 30.75, 123.0, 41.0, 0.97, 123.0, 1.0, 3.0], '20'), ([0.0, 0.0, 2.0, 3.0, 0.0, 0.0, 0.0, 150000.0], '20'))\n",
      "(([1.0, 67.0, 67.0, 18.0, 18.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0], '80'), ([1.0, 101.0, 19.0, 97.0, 0.0, 4.0, 24.25, 0.0, 24.25, 0.96, 97.0, 0.0, 4.0], '80'), ([0.0, 0.0, 2.0, 3.0, 1.0, 0.0, 0.0, 150000.0], '80'))\n",
      "(([1.0, 57.0, 57.0, 10.0, 10.0, 1.0, 1.0, 57.0, 10.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.0], '60'), ([1.0, 91.0, 18.0, 86.0, 1.0, 4.0, 17.2, 86.0, 21.5, 0.95, 86.0, 1.0, 4.0], '60'), ([0.0, 0.0, 2.0, 3.0, 1.0, 0.0, 0.0, 150000.0], '60'))\n",
      "(([1.0, 52.0, 52.0, 31.0, 31.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0], '60'), ([1.0, 85.0, 19.0, 82.0, 0.0, 3.0, 27.33, 0.0, 27.33, 0.96, 82.0, 0.0, 3.0], '60'), ([0.0, 0.0, 2.0, 3.0, 1.0, 0.0, 0.0, 150000.0], '60'))\n",
      "(([1.0, 61.0, 61.0, 11.0, 11.0, 1.0, 1.0, 61.0, 11.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 6.0], '53'), ([1.0, 96.0, 25.0, 91.0, 1.0, 4.0, 18.2, 91.0, 22.75, 0.95, 91.0, 1.0, 4.0], '53'), ([0.0, 0.0, 2.0, 3.0, 0.0, 0.0, 0.0, 150000.0], '53'))\n",
      "(([1.0, 91.0, 91.0, 60.0, 60.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0], '80'), ([1.0, 124.0, 20.0, 121.0, 0.0, 3.0, 40.33, 0.0, 40.33, 0.98, 121.0, 0.0, 3.0], '80'), ([0.0, 0.0, 3.0, 4.0, 0.0, 0.0, 0.0, 150000.0], '80'))\n",
      "(([1.0, 53.0, 53.0, 24.0, 24.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0], '80'), ([1.0, 86.0, 19.0, 83.0, 0.0, 3.0, 27.67, 0.0, 27.67, 0.97, 83.0, 0.0, 3.0], '80'), ([0.0, 0.0, 4.0, 5.0, 0.0, 0.0, 0.0, 150000.0], '80'))\n",
      "(([1.0, 94.0, 94.0, 25.0, 25.0, 1.0, 1.0, 94.0, 25.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.0], '53'), ([1.0, 128.0, 18.0, 124.0, 1.0, 3.0, 31.0, 124.0, 41.33, 0.97, 124.0, 1.0, 3.0], '53'), ([0.0, 0.0, 2.0, 3.0, 0.0, 0.0, 0.0, 150000.0], '53'))\n",
      "(([1.0, 44.0, 44.0, 14.0, 14.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0], '60'), ([1.0, 77.0, 18.0, 74.0, 0.0, 3.0, 24.67, 0.0, 24.67, 0.96, 74.0, 0.0, 3.0], '60'), ([0.0, 0.0, 2.0, 3.0, 0.0, 0.0, 0.0, 150000.0], '60'))\n",
      "(([1.0, 46.0, 46.0, 12.0, 12.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.0], '40'), ([1.0, 75.0, 15.0, 72.0, 0.0, 3.0, 24.0, 0.0, 24.0, 0.96, 72.0, 0.0, 3.0], '40'), ([0.0, 0.0, 2.0, 3.0, 0.0, 0.0, 0.0, 150000.0], '40'))\n",
      "(([1.0, 34.0, 34.0, 6.0, 6.0, 5.0, 5.0, 6.8, 1.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.0], '53'), ([1.0, 72.0, 20.0, 64.0, 5.0, 4.0, 7.11, 12.8, 16.0, 0.89, 64.0, 5.0, 4.0], '53'), ([0.0, 0.0, 2.0, 3.0, 0.0, 0.0, 0.0, 150000.0], '53'))\n",
      "(([1.0, 74.0, 74.0, 16.0, 16.0, 47.0, 47.0, 1.574, 0.34, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], '55'), ([1.0, 149.0, 15.0, 99.0, 47.0, 3.0, 1.98, 2.11, 33.0, 0.66, 99.0, 47.0, 3.0], '55'), ([0.0, 0.0, 2.0, 3.0, 0.0, 0.0, 0.0, 150000.0], '55'))\n",
      "(([1.0, 54.0, 54.0, 10.0, 10.0, 3.0, 3.0, 18.0, 3.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.0], '60'), ([1.0, 90.0, 17.0, 83.0, 3.0, 4.0, 11.86, 27.67, 20.75, 0.92, 83.0, 3.0, 4.0], '60'), ([0.0, 0.0, 2.0, 3.0, 1.0, 0.0, 0.0, 150000.0], '60'))\n",
      "(([1.0, 55.0, 55.0, 16.0, 16.0, 3.0, 3.0, 18.333, 5.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], '6'), ([1.0, 85.0, 16.0, 81.0, 3.0, 1.0, 20.25, 27.0, 81.0, 0.95, 81.0, 3.0, 1.0], '6'), ([0.0, 0.0, 3.0, 4.0, 0.0, 0.0, 0.0, 150000.0], '6'))\n",
      "(([1.0, 94.0, 94.0, 25.0, 25.0, 1.0, 1.0, 94.0, 25.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.0], '53'), ([1.0, 128.0, 18.0, 124.0, 1.0, 3.0, 31.0, 124.0, 41.33, 0.97, 124.0, 1.0, 3.0], '53'), ([0.0, 0.0, 2.0, 3.0, 0.0, 0.0, 0.0, 150000.0], '53'))\n",
      "(([1.0, 115.0, 115.0, 20.0, 20.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 6.0], '40'), ([1.0, 146.0, 14.0, 141.0, 0.0, 5.0, 28.2, 0.0, 28.2, 0.97, 141.0, 0.0, 5.0], '40'), ([0.0, 0.0, 2.0, 3.0, 0.0, 0.0, 0.0, 150000.0], '40'))\n",
      "(([1.0, 97.0, 97.0, 18.0, 18.0, 1.0, 1.0, 97.0, 18.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8.0], '20'), ([1.0, 127.0, 14.0, 123.0, 1.0, 3.0, 30.75, 123.0, 41.0, 0.97, 123.0, 1.0, 3.0], '20'), ([0.0, 0.0, 2.0, 3.0, 0.0, 0.0, 0.0, 150000.0], '20'))\n",
      "(([1.0, 63.0, 63.0, 18.0, 18.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.0], '40'), ([1.0, 92.0, 14.0, 89.0, 0.0, 3.0, 29.67, 0.0, 29.67, 0.97, 89.0, 0.0, 3.0], '40'), ([0.0, 0.0, 2.0, 3.0, 0.0, 0.0, 0.0, 150000.0], '40'))\n",
      "(([1.0, 91.0, 91.0, 60.0, 60.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0], '80'), ([1.0, 124.0, 20.0, 121.0, 0.0, 3.0, 40.33, 0.0, 40.33, 0.98, 121.0, 0.0, 3.0], '80'), ([0.0, 0.0, 3.0, 4.0, 0.0, 0.0, 0.0, 150000.0], '80'))\n",
      "(([1.0, 63.0, 63.0, 18.0, 18.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.0], '40'), ([1.0, 92.0, 14.0, 89.0, 0.0, 3.0, 29.67, 0.0, 29.67, 0.97, 89.0, 0.0, 3.0], '40'), ([0.0, 0.0, 2.0, 3.0, 0.0, 0.0, 0.0, 150000.0], '40'))\n"
     ]
    }
   ],
   "source": [
    "print (len(src_vec_map[\"4TShirt\"]))\n",
    "for v in src_vec_map[\"4TShirt\"] :\n",
    "    print (v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1822\n",
      "('Number of samples: ', 18667)\n"
     ]
    }
   ],
   "source": [
    "print (len(src_vec_map))\n",
    "sample_count = 0\n",
    "for k in src_vec_map:\n",
    "    sample_count += len(src_vec_map[k])\n",
    "    \n",
    "print (\"Number of samples: \", sample_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def load_dataset(src_vec_map):\n",
    "    dataset_list = []\n",
    "    \n",
    "    for k in src_vec_map:\n",
    "        for i in range(len(src_vec_map[k])):\n",
    "            label = src_vec_map[k][i][0][1]\n",
    "            l1 = np.asarray(src_vec_map[k][i][0][0])\n",
    "            l2 = np.asarray(src_vec_map[k][i][1][0])\n",
    "            l3 = np.asarray(src_vec_map[k][i][2][0])\n",
    "            l = np.concatenate((l1, l2), axis=0)\n",
    "            l = np.concatenate((l, l3), axis=0)\n",
    "            dataset_list.append((l, (k, label)))\n",
    "    dataset = np.array(dataset_list)\n",
    "    dataset = dataset.reshape((dataset.shape[0], dataset.shape[1]))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18667, 2)\n",
      "[ array([  1.00000000e+00,   7.90000000e+01,   7.90000000e+01,\n",
      "         6.10000000e+01,   6.10000000e+01,   0.00000000e+00,\n",
      "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "         4.00000000e+00,   1.00000000e+00,   8.00000000e+01,\n",
      "         1.60000000e+01,   8.00000000e+01,   0.00000000e+00,\n",
      "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "         0.00000000e+00,   1.00000000e+00,   8.00000000e+01,\n",
      "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "         0.00000000e+00,   3.00000000e+00,   4.00000000e+00,\n",
      "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "         1.50000000e+05])\n",
      " ('bodmas', '80')]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(src_vec_map)\n",
    "print (dataset.shape)\n",
    "\n",
    "print (dataset[0])\n",
    "\n",
    "#print dataset[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_author_features_and_scores(author_name, dataset):\n",
    "    codes = []\n",
    "    scores = []\n",
    "    \n",
    "    for item in dataset:\n",
    "        if item[1][0] == author_name:\n",
    "            codes.append(item[0])\n",
    "            scores.append(item[1][1])\n",
    "            \n",
    "    return (codes, scores)  \n",
    "\n",
    "def create_correct_np_array(data_list):\n",
    "    arr = np.array([])\n",
    "    arr = np.hstack((arr, np.array(data_list[0])))\n",
    "\n",
    "    for i in range(1, len(data_list)):\n",
    "        arr = np.vstack((arr, np.array(data_list[i])))\n",
    "        \n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1300\n",
      "522\n"
     ]
    }
   ],
   "source": [
    "# training and test authors, cca 70-30 omjer\n",
    "author_names = list(src_vec_map.keys())\n",
    "\n",
    "train_authors = author_names[0:1300]\n",
    "test_authors = author_names[1300:]\n",
    "\n",
    "print(len(train_authors))\n",
    "print(len(test_authors))\n",
    "\n",
    "X_train_list = []\n",
    "y_train_list = []\n",
    "X_test_list = []\n",
    "y_test_list = []\n",
    "\n",
    "for author in train_authors:\n",
    "    codes, scores = get_author_features_and_scores(author, dataset)\n",
    "    X_train_list.extend(codes)\n",
    "    y_train_list.extend(scores)\n",
    "    \n",
    "for author in test_authors:\n",
    "    codes, scores = get_author_features_and_scores(author, dataset)\n",
    "    X_test_list.extend(codes)\n",
    "    y_test_list.extend(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13493, 37)\n",
      "(5174, 37)\n",
      "(13493, 1)\n",
      "(5174, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train = create_correct_np_array(X_train_list)\n",
    "X_test = create_correct_np_array(X_test_list)\n",
    "y_train = create_correct_np_array(y_train_list)\n",
    "y_test = create_correct_np_array(y_test_list)\n",
    "\n",
    "print (X_train.shape)\n",
    "print (X_test.shape)\n",
    "print (y_train.shape)\n",
    "print (y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['Files', 'Lines', 'AVG Len', 'Code', 'Comments', 'White SP', 'Cd/Cm+WS', 'Cd/Cm', 'Cd/WS', '% Code', 'Cd/File', 'Cm/File', 'WS/File'], '\\n')\n",
      "(['number_of_modules', 'lines_of_code', 'lines_of_code_per_module', 'McCabes_cyclomatic_complexity', 'McCabes_cyclomatic_complexity_per_module', 'lines_of_comment', 'lines_of_comment_per_module', 'lines_of_code_per_line_of_comment', 'McCabes_cyclomatic_complexity_per_line_of_comment', 'IF4', 'IF4_per_module', 'IF4_visible', 'IF4_visible_per_module', 'IF4_concrete', 'IF4_concrete', 'rejected_lines_of_code\\n'], '\\n')\n",
      "['cnt_classes', 'max_member_funs', 'max_nested_loops', 'max_nesting_depth', 'max_params_in_decl', 'member_funs', 'member_vars']\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "print (codeAnalyzerHeader.split(',')[1:-1], \"\\n\")\n",
    "print (ccccHeader.split(';')[1:], \"\\n\")\n",
    "print (ASTExtractorHeader.split(';')[1:-1])\n",
    "labels = ccccHeader.split(';')[1:] + codeAnalyzerHeader.split(',')[1:-1] + ASTExtractorHeader.split(';')[1:]\n",
    "\n",
    "print (len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13493, 37)\n"
     ]
    }
   ],
   "source": [
    "print (X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 29, 30, 31, 32, 33, 34, 35, 36]\n",
      "(13493, 34)\n",
      "(5174, 34)\n",
      "['number_of_modules', 'lines_of_code', 'lines_of_code_per_module', 'McCabes_cyclomatic_complexity', 'McCabes_cyclomatic_complexity_per_module', 'lines_of_comment', 'lines_of_comment_per_module', 'lines_of_code_per_line_of_comment', 'McCabes_cyclomatic_complexity_per_line_of_comment', 'IF4', 'IF4_per_module', 'IF4_visible', 'IF4_visible_per_module', 'IF4_concrete', 'IF4_concrete', 'rejected_lines_of_code\\n', 'Files', 'Lines', 'AVG Len', 'Code', 'Comments', 'White SP', 'Cd/Cm+WS', 'Cd/Cm', 'Cd/WS', '% Code', 'cnt_classes', 'max_member_funs', 'max_nested_loops', 'max_nesting_depth', 'max_params_in_decl', 'member_funs', 'member_vars', 'min_member_funs\\n']\n"
     ]
    }
   ],
   "source": [
    "def enlist_kept_cols(X, labels):\n",
    "    \n",
    "    cnt_cols = X.shape[1]\n",
    "    X_reduced = []\n",
    "    #labels_reduced = []\n",
    "    kept_cols = []\n",
    "    \n",
    "    for i in range(cnt_cols):\n",
    "        col = X[:,i]\n",
    "        notInside = True\n",
    "        for j in range(len(X_reduced)):\n",
    "            if (col == X_reduced[j]).all():\n",
    "                notInside = False\n",
    "        \n",
    "        if notInside:\n",
    "            kept_cols.append(i)\n",
    "            #labels_reduced.append(labels[i])\n",
    "            X_reduced.append(col)\n",
    "\n",
    "    #return np.array(X_reduced).T, labels_reduced, kept_cols\n",
    "    return kept_cols\n",
    "\n",
    "#X, labels_reduced = remove_same_cols(X, labels)\n",
    "# decision on which column will be kept or not is made on training part because it is bigger\n",
    "print len(labels)\n",
    "kept_cols = enlist_kept_cols(X_train, labels)\n",
    "print (kept_cols)\n",
    "X_train_reduced = X_train[:, kept_cols]\n",
    "print (X_train_reduced.shape)\n",
    "X_test_reduced = X_test[:, kept_cols]\n",
    "print (X_test_reduced.shape)\n",
    "labels_reduced = [labels[i] for i in kept_cols]\n",
    "print (labels_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "def create_json(author_names, dataset):\n",
    "    data = {}\n",
    "    data[\"column_descriptors\"] = labels_reduced\n",
    "    data[\"author_data\"] = {}\n",
    "    for author in author_names:\n",
    "        feature_vecs, scores = get_author_features_and_scores(author, dataset)\n",
    "        if len(scores) == 0:\n",
    "            continue\n",
    "        feat_vecs = [fv[kept_cols].tolist() for fv in feature_vecs]\n",
    "        author_data = {}\n",
    "        author_data[\"feature_vecs\"] = feat_vecs\n",
    "        author_data[\"scores\"] = scores\n",
    "        data[\"author_data\"][author] = author_data\n",
    "        \n",
    "    with open('dataset.json', 'w') as outfile:\n",
    "        json.dump(data, outfile)\n",
    "\n",
    "#print get_author_features_and_scores(author_names[15], dataset)[1]\n",
    "create_json(author_names, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
